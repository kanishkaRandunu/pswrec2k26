{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SASRec training curves â€“ TensorBoard logs & job output\n",
        "\n",
        "Plot training loss and validation metrics (Hit@10, NDCG@10) from:\n",
        "1. **TensorBoard event logs** (`log_tensorboard/`)\n",
        "2. **Job text logs** (PBS `.o` file or `sasrec_live.log`) as fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Paths: TRACT repo root (notebooks/ is one level below)\n",
        "TRACT_ROOT = Path(os.getcwd()).resolve()\n",
        "if (TRACT_ROOT / \"log_tensorboard\").exists():\n",
        "    pass  # already in TRACT root\n",
        "elif (TRACT_ROOT.parent / \"log_tensorboard\").exists():\n",
        "    TRACT_ROOT = TRACT_ROOT.parent  # run from notebooks/\n",
        "else:\n",
        "    TRACT_ROOT = Path(\"..\").resolve()  # fallback\n",
        "LOG_TENSORBOARD = TRACT_ROOT / \"log_tensorboard\"\n",
        "LOG_JOB = TRACT_ROOT / \"sasrec_live.log\"  # or e.g. sasrec_ml1m.o159742817"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load from TensorBoard event logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "    HAS_TENSORBOARD = True\n",
        "except ImportError:\n",
        "    HAS_TENSORBOARD = False\n",
        "    print(\"Install tensorboard: pip install tensorboard\")\n",
        "\n",
        "def load_tensorboard_scalars(logdir):\n",
        "    \"\"\"Load all scalar runs from a TensorBoard log directory (parent of run dirs).\"\"\"\n",
        "    if not HAS_TENSORBOARD or not logdir.exists():\n",
        "        return {}\n",
        "    runs = {}\n",
        "    for run_path in sorted(logdir.iterdir()):\n",
        "        if not run_path.is_dir():\n",
        "            continue\n",
        "        ea = EventAccumulator(str(run_path))\n",
        "        ea.Reload()\n",
        "        tags = ea.Tags().get(\"scalars\", [])\n",
        "        if not tags:\n",
        "            continue\n",
        "        runs[run_path.name] = {}\n",
        "        for tag in tags:\n",
        "            events = ea.Scalars(tag)\n",
        "            steps = [e.step for e in events]\n",
        "            values = [e.value for e in events]\n",
        "            runs[run_path.name][tag] = (np.array(steps), np.array(values))\n",
        "    return runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tb_runs = load_tensorboard_scalars(LOG_TENSORBOARD)\n",
        "print(\"TensorBoard runs found:\", list(tb_runs.keys()))\n",
        "for run_name, scalars in tb_runs.items():\n",
        "    print(f\"  {run_name}: {list(scalars.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tensorboard_runs(tb_runs, tags_to_plot=None):\n",
        "    \"\"\"Plot scalar curves for each run. tags_to_plot: list of tag names, or None = all.\"\"\"\n",
        "    if not tb_runs:\n",
        "        print(\"No TensorBoard runs to plot.\")\n",
        "        return\n",
        "    all_tags = set()\n",
        "    for scalars in tb_runs.values():\n",
        "        all_tags.update(scalars.keys())\n",
        "    tags = tags_to_plot or sorted(all_tags)\n",
        "    n = len(tags)\n",
        "    fig, axes = plt.subplots(1, n, figsize=(5 * n, 4))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    for ax, tag in zip(axes, tags):\n",
        "        for run_name, scalars in tb_runs.items():\n",
        "            if tag not in scalars:\n",
        "                continue\n",
        "            steps, values = scalars[tag]\n",
        "            ax.plot(steps, values, label=run_name[:40], alpha=0.8)\n",
        "        ax.set_title(tag)\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.legend(loc=\"best\", fontsize=7)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_tensorboard_runs(tb_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Parse job text log (fallback / comparison)\n",
        "\n",
        "Extract epoch, train loss, Hit@10, NDCG@10 from lines like:\n",
        "- `epoch 43 training [time: 21.92s, train loss: 1119.2058]`\n",
        "- `hit@10 : 0.2456    ndcg@10 : 0.1256`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_job_log(log_path):\n",
        "    \"\"\"Parse PBS/job log; return dict with epoch, train_loss, hit10, ndcg10 arrays.\"\"\"\n",
        "    log_path = Path(log_path)\n",
        "    if not log_path.exists():\n",
        "        return None\n",
        "    text = log_path.read_text()\n",
        "    epochs, train_losses, hit10, ndcg10 = [], [], [], []\n",
        "    # epoch N training [..., train loss: XXXX]\n",
        "    for m in re.finditer(r\"epoch\\s+(\\d+)\\s+training\\s+.*train loss:\\s+([\\d.]+)\", text):\n",
        "        epochs.append(int(m.group(1)))\n",
        "        train_losses.append(float(m.group(2)))\n",
        "    # hit@10 : X.XXXX    ndcg@10 : X.XXXX (after 'valid result:')\n",
        "    for m in re.finditer(r\"hit@10\\s*:\\s*([\\d.]+)\\s*ndcg@10\\s*:\\s*([\\d.]+)\", text, re.IGNORECASE):\n",
        "        hit10.append(float(m.group(1)))\n",
        "        ndcg10.append(float(m.group(2)))\n",
        "    # Align length (valid every epoch)\n",
        "    n = min(len(epochs), len(hit10), len(ndcg10))\n",
        "    if n == 0:\n",
        "        return None\n",
        "    return {\n",
        "        \"epoch\": np.array(epochs[:n]),\n",
        "        \"train_loss\": np.array(train_losses[:n]),\n",
        "        \"hit@10\": np.array(hit10[:n]),\n",
        "        \"ndcg@10\": np.array(ndcg10[:n]),\n",
        "    }\n",
        "\n",
        "# Try default live log; or set path to a specific .o file\n",
        "job_log_path = LOG_JOB\n",
        "if not job_log_path.exists():\n",
        "    # Try any sasrec_ml1m.o* in TRACT root\n",
        "    for f in TRACT_ROOT.glob(\"sasrec_ml1m.o*\"):\n",
        "        job_log_path = f\n",
        "        break\n",
        "parsed = parse_job_log(job_log_path)\n",
        "print(\"Job log path:\", job_log_path)\n",
        "print(\"Parsed:\", parsed if parsed is None else {k: v.shape for k, v in parsed.items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_job_log(parsed, title=\"Training curves (job log)\"):\n",
        "    if parsed is None:\n",
        "        print(\"No parsed job log to plot.\")\n",
        "        return\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "    ep = parsed[\"epoch\"]\n",
        "    axes[0].plot(ep, parsed[\"train_loss\"], \"b-\", label=\"Train loss\")\n",
        "    axes[0].set_title(\"Train loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1].plot(ep, parsed[\"hit@10\"], \"g-\", label=\"Hit@10\")\n",
        "    axes[1].set_title(\"Valid Hit@10\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[2].plot(ep, parsed[\"ndcg@10\"], \"orange\", label=\"NDCG@10\")\n",
        "    axes[2].set_title(\"Valid NDCG@10\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_job_log(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optional: plot a specific PBS output file\n",
        "\n",
        "Set `job_id` to your job ID (e.g. 159742817) to plot that run's log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_id = \"159742817\"  # change to your job ID\n",
        "job_o_file = TRACT_ROOT / f\"sasrec_ml1m.o{job_id}\"\n",
        "parsed_specific = parse_job_log(job_o_file)\n",
        "if parsed_specific is not None:\n",
        "    plot_job_log(parsed_specific, title=f\"SASRec ml-1m (job {job_id})\")\n",
        "else:\n",
        "    print(f\"File not found or no matches: {job_o_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
