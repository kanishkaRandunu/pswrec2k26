n_layers: 2                     # (int) Number of transformer layers in encoder.
n_heads: 2                      # (int) Number of attention heads.
hidden_size: 64                 # (int) Hidden state / embedding size.
inner_size: 256                 # (int) Inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) Dropout probability (hidden).
attn_dropout_prob: 0.5          # (float) Dropout probability (attention).
hidden_act: 'gelu'              # (str) Activation in feed-forward layer.
layer_norm_eps: 1e-12           # (float) LayerNorm epsilon.
initializer_range: 0.02         # (float) Std for normal initialization.
loss_type: 'CE'                 # (str) Loss function. Range in ['BPR', 'CE'].

# Local phase filterbank (timeâ€“frequency prism).
n_bands: 4                      # (int) Number of local spectral bands (derived from kernel list if overridden).
band_kernel_sizes: [3, 7, 15, 31]   # (list[int]) Kernel sizes for each band.
band_dilations: [1, 1, 1, 1]        # (list[int]) Dilation for each band.

# Phase-synchrony attention bias.
phase_bias_scale: 0.1           # (float) Initial scale for per-head phase bias.

# Optional auxiliary phase smoothness regularization (off by default).
phase_aux: False                # (bool) Enable phase smoothness penalty.
phase_aux_weight: 0.0           # (float) Weight of phase smoothness loss term.

