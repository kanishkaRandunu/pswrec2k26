command line args [-m BERT4Rec -d amazon-beauty --config_files configs/beauty/beauty_BERT4Rec.yaml] will not be used in RecBole
11 Feb 22:59    INFO  ['RecBole/run_recbole.py', '-m', 'BERT4Rec', '-d', 'amazon-beauty', '--config_files', 'configs/beauty/beauty_BERT4Rec.yaml']
11 Feb 22:59    INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2026
state = INFO
reproducibility = True
data_path = /scratch/up63/kd6504/recbole_datasets/amazon-beauty
checkpoint_dir = outputs/bert4rec_beauty
show_progress = False
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 200
train_batch_size = 128
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = True
metrics = ['Hit', 'NDCG', 'MRR']
topk = [1, 5, 10, 20]
valid_metric = NDCG@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = {'inter': ['rating']}
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = None

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = mask_itemseq
n_layers = 2
n_heads = 2
hidden_size = 64
inner_size = 256
hidden_dropout_prob = 0.2
attn_dropout_prob = 0.2
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
mask_ratio = 0.2
loss_type = CE
ft_ratio = 0.5
numerical_features = []
discretization = None
kg_reverse_r = False
entity_kg_num_interval = [0,inf)
relation_kg_num_interval = [0,inf)
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cuda
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:648: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=0, inplace=True)
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:650: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=feat[field].mean(), inplace=True)
11 Feb 23:00    INFO  amazon-beauty
The number of users: 22364
Average actions of users: 8.876358270357287
The number of items: 12102
Average actions of items: 16.403768283612923
The number of inters: 198502
The sparsity of the dataset: 99.92665707018277%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp']
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:2202: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  new_data[k] = torch.LongTensor(value)
11 Feb 23:00    INFO  [Training]: train_batch_size = [128] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
11 Feb 23:00    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
11 Feb 23:00    INFO  BERT4Rec(
  (item_embedding): Embedding(12103, 64, padding_idx=0)
  (position_embedding): Embedding(50, 64)
  (trm_encoder): TransformerEncoder(
    (layer): ModuleList(
      (0-1): 2 x TransformerLayer(
        (multi_head_attention): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.2, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.2, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (output_ffn): Linear(in_features=64, out_features=64, bias=True)
  (output_gelu): GELU(approximate='none')
  (output_ln): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
)
Trainable parameters: 894278
11 Feb 23:00    INFO  FLOPs: 5194664.0
/home/572/kd6504/TRACT/RecBole/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = amp.GradScaler(enabled=self.enable_scaler)
11 Feb 23:00    INFO  epoch 0 training [time: 45.35s, train loss: 8392.1442]
11 Feb 23:00    INFO  epoch 0 evaluating [time: 1.52s, valid_score: 0.016100]
11 Feb 23:00    INFO  valid result: 
hit@1 : 0.0052    hit@5 : 0.0188    hit@10 : 0.032    hit@20 : 0.0511    ndcg@1 : 0.0052    ndcg@5 : 0.0118    ndcg@10 : 0.0161    ndcg@20 : 0.0208    mrr@1 : 0.0052    mrr@5 : 0.0096    mrr@10 : 0.0113    mrr@20 : 0.0126
11 Feb 23:00    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:01    INFO  epoch 1 training [time: 45.02s, train loss: 7605.5127]
11 Feb 23:01    INFO  epoch 1 evaluating [time: 1.50s, valid_score: 0.019100]
11 Feb 23:01    INFO  valid result: 
hit@1 : 0.0055    hit@5 : 0.022    hit@10 : 0.0389    hit@20 : 0.0643    ndcg@1 : 0.0055    ndcg@5 : 0.0137    ndcg@10 : 0.0191    ndcg@20 : 0.0255    mrr@1 : 0.0055    mrr@5 : 0.011    mrr@10 : 0.0132    mrr@20 : 0.015
11 Feb 23:01    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:02    INFO  epoch 2 training [time: 45.13s, train loss: 7272.0263]
11 Feb 23:02    INFO  epoch 2 evaluating [time: 1.50s, valid_score: 0.022000]
11 Feb 23:02    INFO  valid result: 
hit@1 : 0.006    hit@5 : 0.027    hit@10 : 0.0444    hit@20 : 0.0711    ndcg@1 : 0.006    ndcg@5 : 0.0164    ndcg@10 : 0.022    ndcg@20 : 0.0287    mrr@1 : 0.006    mrr@5 : 0.0129    mrr@10 : 0.0152    mrr@20 : 0.017
11 Feb 23:02    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:03    INFO  epoch 3 training [time: 45.26s, train loss: 7044.2580]
11 Feb 23:03    INFO  epoch 3 evaluating [time: 1.50s, valid_score: 0.026100]
11 Feb 23:03    INFO  valid result: 
hit@1 : 0.0089    hit@5 : 0.0316    hit@10 : 0.0506    hit@20 : 0.0811    ndcg@1 : 0.0089    ndcg@5 : 0.02    ndcg@10 : 0.0261    ndcg@20 : 0.0337    mrr@1 : 0.0089    mrr@5 : 0.0162    mrr@10 : 0.0187    mrr@20 : 0.0208
11 Feb 23:03    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:04    INFO  epoch 4 training [time: 44.51s, train loss: 6831.9649]
11 Feb 23:04    INFO  epoch 4 evaluating [time: 1.45s, valid_score: 0.026100]
11 Feb 23:04    INFO  valid result: 
hit@1 : 0.0074    hit@5 : 0.0313    hit@10 : 0.0528    hit@20 : 0.0836    ndcg@1 : 0.0074    ndcg@5 : 0.0192    ndcg@10 : 0.0261    ndcg@20 : 0.0338    mrr@1 : 0.0074    mrr@5 : 0.0152    mrr@10 : 0.018    mrr@20 : 0.0201
11 Feb 23:04    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:04    INFO  epoch 5 training [time: 44.48s, train loss: 6636.7447]
11 Feb 23:04    INFO  epoch 5 evaluating [time: 1.48s, valid_score: 0.027200]
11 Feb 23:04    INFO  valid result: 
hit@1 : 0.0078    hit@5 : 0.0333    hit@10 : 0.0544    hit@20 : 0.0857    ndcg@1 : 0.0078    ndcg@5 : 0.0205    ndcg@10 : 0.0272    ndcg@20 : 0.0351    mrr@1 : 0.0078    mrr@5 : 0.0163    mrr@10 : 0.0191    mrr@20 : 0.0212
11 Feb 23:04    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:05    INFO  epoch 6 training [time: 44.82s, train loss: 6486.7077]
11 Feb 23:05    INFO  epoch 6 evaluating [time: 1.48s, valid_score: 0.027900]
11 Feb 23:05    INFO  valid result: 
hit@1 : 0.0078    hit@5 : 0.0345    hit@10 : 0.0555    hit@20 : 0.0855    ndcg@1 : 0.0078    ndcg@5 : 0.0211    ndcg@10 : 0.0279    ndcg@20 : 0.0354    mrr@1 : 0.0078    mrr@5 : 0.0168    mrr@10 : 0.0195    mrr@20 : 0.0216
11 Feb 23:05    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:06    INFO  epoch 7 training [time: 43.97s, train loss: 6273.7024]
11 Feb 23:06    INFO  epoch 7 evaluating [time: 1.45s, valid_score: 0.027600]
11 Feb 23:06    INFO  valid result: 
hit@1 : 0.0081    hit@5 : 0.033    hit@10 : 0.055    hit@20 : 0.0862    ndcg@1 : 0.0081    ndcg@5 : 0.0205    ndcg@10 : 0.0276    ndcg@20 : 0.0355    mrr@1 : 0.0081    mrr@5 : 0.0165    mrr@10 : 0.0194    mrr@20 : 0.0216
11 Feb 23:07    INFO  epoch 8 training [time: 44.03s, train loss: 6201.8332]
11 Feb 23:07    INFO  epoch 8 evaluating [time: 1.46s, valid_score: 0.028700]
11 Feb 23:07    INFO  valid result: 
hit@1 : 0.0085    hit@5 : 0.0359    hit@10 : 0.0573    hit@20 : 0.0882    ndcg@1 : 0.0085    ndcg@5 : 0.0219    ndcg@10 : 0.0287    ndcg@20 : 0.0365    mrr@1 : 0.0085    mrr@5 : 0.0173    mrr@10 : 0.0201    mrr@20 : 0.0222
11 Feb 23:07    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:07    INFO  epoch 9 training [time: 43.83s, train loss: 6112.3876]
11 Feb 23:07    INFO  epoch 9 evaluating [time: 1.45s, valid_score: 0.028800]
11 Feb 23:07    INFO  valid result: 
hit@1 : 0.0087    hit@5 : 0.0346    hit@10 : 0.0566    hit@20 : 0.0881    ndcg@1 : 0.0087    ndcg@5 : 0.0217    ndcg@10 : 0.0288    ndcg@20 : 0.0368    mrr@1 : 0.0087    mrr@5 : 0.0175    mrr@10 : 0.0204    mrr@20 : 0.0226
11 Feb 23:07    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:08    INFO  epoch 10 training [time: 43.88s, train loss: 6023.0157]
11 Feb 23:08    INFO  epoch 10 evaluating [time: 1.48s, valid_score: 0.029200]
11 Feb 23:08    INFO  valid result: 
hit@1 : 0.0096    hit@5 : 0.0354    hit@10 : 0.0563    hit@20 : 0.0875    ndcg@1 : 0.0096    ndcg@5 : 0.0225    ndcg@10 : 0.0292    ndcg@20 : 0.0371    mrr@1 : 0.0096    mrr@5 : 0.0183    mrr@10 : 0.021    mrr@20 : 0.0232
11 Feb 23:08    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:09    INFO  epoch 11 training [time: 43.98s, train loss: 5893.8488]
11 Feb 23:09    INFO  epoch 11 evaluating [time: 1.45s, valid_score: 0.028800]
11 Feb 23:09    INFO  valid result: 
hit@1 : 0.0095    hit@5 : 0.0356    hit@10 : 0.0551    hit@20 : 0.0841    ndcg@1 : 0.0095    ndcg@5 : 0.0226    ndcg@10 : 0.0288    ndcg@20 : 0.0361    mrr@1 : 0.0095    mrr@5 : 0.0184    mrr@10 : 0.0209    mrr@20 : 0.0229
11 Feb 23:10    INFO  epoch 12 training [time: 43.74s, train loss: 5872.1543]
11 Feb 23:10    INFO  epoch 12 evaluating [time: 1.47s, valid_score: 0.029200]
11 Feb 23:10    INFO  valid result: 
hit@1 : 0.009    hit@5 : 0.0351    hit@10 : 0.057    hit@20 : 0.0874    ndcg@1 : 0.009    ndcg@5 : 0.0221    ndcg@10 : 0.0292    ndcg@20 : 0.0369    mrr@1 : 0.009    mrr@5 : 0.0179    mrr@10 : 0.0208    mrr@20 : 0.0229
11 Feb 23:10    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:10    INFO  epoch 13 training [time: 44.94s, train loss: 5749.9161]
11 Feb 23:10    INFO  epoch 13 evaluating [time: 1.43s, valid_score: 0.029900]
11 Feb 23:10    INFO  valid result: 
hit@1 : 0.0099    hit@5 : 0.0362    hit@10 : 0.0571    hit@20 : 0.0888    ndcg@1 : 0.0099    ndcg@5 : 0.0232    ndcg@10 : 0.0299    ndcg@20 : 0.0378    mrr@1 : 0.0099    mrr@5 : 0.0189    mrr@10 : 0.0216    mrr@20 : 0.0238
11 Feb 23:10    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:11    INFO  epoch 14 training [time: 44.74s, train loss: 5695.4778]
11 Feb 23:11    INFO  epoch 14 evaluating [time: 1.47s, valid_score: 0.029700]
11 Feb 23:11    INFO  valid result: 
hit@1 : 0.0094    hit@5 : 0.0359    hit@10 : 0.0583    hit@20 : 0.0881    ndcg@1 : 0.0094    ndcg@5 : 0.0225    ndcg@10 : 0.0297    ndcg@20 : 0.0372    mrr@1 : 0.0094    mrr@5 : 0.0181    mrr@10 : 0.0211    mrr@20 : 0.0231
11 Feb 23:12    INFO  epoch 15 training [time: 44.83s, train loss: 5646.8061]
11 Feb 23:12    INFO  epoch 15 evaluating [time: 1.45s, valid_score: 0.029500]
11 Feb 23:12    INFO  valid result: 
hit@1 : 0.0102    hit@5 : 0.0357    hit@10 : 0.0559    hit@20 : 0.0856    ndcg@1 : 0.0102    ndcg@5 : 0.023    ndcg@10 : 0.0295    ndcg@20 : 0.037    mrr@1 : 0.0102    mrr@5 : 0.0188    mrr@10 : 0.0215    mrr@20 : 0.0235
11 Feb 23:13    INFO  epoch 16 training [time: 44.96s, train loss: 5570.1612]
11 Feb 23:13    INFO  epoch 16 evaluating [time: 1.47s, valid_score: 0.029800]
11 Feb 23:13    INFO  valid result: 
hit@1 : 0.0098    hit@5 : 0.0365    hit@10 : 0.0573    hit@20 : 0.0865    ndcg@1 : 0.0098    ndcg@5 : 0.0231    ndcg@10 : 0.0298    ndcg@20 : 0.0371    mrr@1 : 0.0098    mrr@5 : 0.0187    mrr@10 : 0.0214    mrr@20 : 0.0234
11 Feb 23:13    INFO  epoch 17 training [time: 44.71s, train loss: 5577.0844]
11 Feb 23:14    INFO  epoch 17 evaluating [time: 1.45s, valid_score: 0.029000]
11 Feb 23:14    INFO  valid result: 
hit@1 : 0.0093    hit@5 : 0.0348    hit@10 : 0.0563    hit@20 : 0.0844    ndcg@1 : 0.0093    ndcg@5 : 0.0221    ndcg@10 : 0.029    ndcg@20 : 0.0361    mrr@1 : 0.0093    mrr@5 : 0.018    mrr@10 : 0.0208    mrr@20 : 0.0227
11 Feb 23:14    INFO  epoch 18 training [time: 44.53s, train loss: 5463.1093]
11 Feb 23:14    INFO  epoch 18 evaluating [time: 1.44s, valid_score: 0.030300]
11 Feb 23:14    INFO  valid result: 
hit@1 : 0.0099    hit@5 : 0.0368    hit@10 : 0.0583    hit@20 : 0.0871    ndcg@1 : 0.0099    ndcg@5 : 0.0234    ndcg@10 : 0.0303    ndcg@20 : 0.0376    mrr@1 : 0.0099    mrr@5 : 0.019    mrr@10 : 0.0219    mrr@20 : 0.0239
11 Feb 23:14    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:15    INFO  epoch 19 training [time: 44.19s, train loss: 5433.7798]
11 Feb 23:15    INFO  epoch 19 evaluating [time: 1.45s, valid_score: 0.029300]
11 Feb 23:15    INFO  valid result: 
hit@1 : 0.0099    hit@5 : 0.0349    hit@10 : 0.0564    hit@20 : 0.0859    ndcg@1 : 0.0099    ndcg@5 : 0.0224    ndcg@10 : 0.0293    ndcg@20 : 0.0368    mrr@1 : 0.0099    mrr@5 : 0.0183    mrr@10 : 0.0212    mrr@20 : 0.0232
11 Feb 23:16    INFO  epoch 20 training [time: 44.25s, train loss: 5401.5311]
11 Feb 23:16    INFO  epoch 20 evaluating [time: 1.48s, valid_score: 0.028600]
11 Feb 23:16    INFO  valid result: 
hit@1 : 0.0089    hit@5 : 0.0334    hit@10 : 0.0555    hit@20 : 0.0839    ndcg@1 : 0.0089    ndcg@5 : 0.0215    ndcg@10 : 0.0286    ndcg@20 : 0.0357    mrr@1 : 0.0089    mrr@5 : 0.0176    mrr@10 : 0.0205    mrr@20 : 0.0224
11 Feb 23:17    INFO  epoch 21 training [time: 44.57s, train loss: 5325.2377]
11 Feb 23:17    INFO  epoch 21 evaluating [time: 1.45s, valid_score: 0.028700]
11 Feb 23:17    INFO  valid result: 
hit@1 : 0.0101    hit@5 : 0.0345    hit@10 : 0.054    hit@20 : 0.0825    ndcg@1 : 0.0101    ndcg@5 : 0.0225    ndcg@10 : 0.0287    ndcg@20 : 0.036    mrr@1 : 0.0101    mrr@5 : 0.0185    mrr@10 : 0.0211    mrr@20 : 0.0231
11 Feb 23:17    INFO  epoch 22 training [time: 44.29s, train loss: 5316.2569]
11 Feb 23:17    INFO  epoch 22 evaluating [time: 1.47s, valid_score: 0.029800]
11 Feb 23:17    INFO  valid result: 
hit@1 : 0.0098    hit@5 : 0.0349    hit@10 : 0.0584    hit@20 : 0.0855    ndcg@1 : 0.0098    ndcg@5 : 0.0223    ndcg@10 : 0.0298    ndcg@20 : 0.0366    mrr@1 : 0.0098    mrr@5 : 0.0182    mrr@10 : 0.0213    mrr@20 : 0.0231
11 Feb 23:18    INFO  epoch 23 training [time: 44.46s, train loss: 5269.4130]
11 Feb 23:18    INFO  epoch 23 evaluating [time: 1.44s, valid_score: 0.029900]
11 Feb 23:18    INFO  valid result: 
hit@1 : 0.011    hit@5 : 0.0355    hit@10 : 0.0562    hit@20 : 0.0859    ndcg@1 : 0.011    ndcg@5 : 0.0233    ndcg@10 : 0.0299    ndcg@20 : 0.0373    mrr@1 : 0.011    mrr@5 : 0.0193    mrr@10 : 0.0219    mrr@20 : 0.024
11 Feb 23:19    INFO  epoch 24 training [time: 44.20s, train loss: 5224.0188]
11 Feb 23:19    INFO  epoch 24 evaluating [time: 1.46s, valid_score: 0.029000]
11 Feb 23:19    INFO  valid result: 
hit@1 : 0.0102    hit@5 : 0.0341    hit@10 : 0.055    hit@20 : 0.0841    ndcg@1 : 0.0102    ndcg@5 : 0.0223    ndcg@10 : 0.029    ndcg@20 : 0.0363    mrr@1 : 0.0102    mrr@5 : 0.0184    mrr@10 : 0.0211    mrr@20 : 0.0231
11 Feb 23:20    INFO  epoch 25 training [time: 44.15s, train loss: 5226.5591]
11 Feb 23:20    INFO  epoch 25 evaluating [time: 1.46s, valid_score: 0.029000]
11 Feb 23:20    INFO  valid result: 
hit@1 : 0.01    hit@5 : 0.0359    hit@10 : 0.0551    hit@20 : 0.0843    ndcg@1 : 0.01    ndcg@5 : 0.0228    ndcg@10 : 0.029    ndcg@20 : 0.0363    mrr@1 : 0.01    mrr@5 : 0.0185    mrr@10 : 0.021    mrr@20 : 0.023
11 Feb 23:20    INFO  epoch 26 training [time: 44.41s, train loss: 5226.4943]
11 Feb 23:20    INFO  epoch 26 evaluating [time: 1.44s, valid_score: 0.029500]
11 Feb 23:20    INFO  valid result: 
hit@1 : 0.0097    hit@5 : 0.0351    hit@10 : 0.0563    hit@20 : 0.086    ndcg@1 : 0.0097    ndcg@5 : 0.0227    ndcg@10 : 0.0295    ndcg@20 : 0.0369    mrr@1 : 0.0097    mrr@5 : 0.0186    mrr@10 : 0.0214    mrr@20 : 0.0234
11 Feb 23:21    INFO  epoch 27 training [time: 44.22s, train loss: 5208.7542]
11 Feb 23:21    INFO  epoch 27 evaluating [time: 1.44s, valid_score: 0.029700]
11 Feb 23:21    INFO  valid result: 
hit@1 : 0.0099    hit@5 : 0.036    hit@10 : 0.0574    hit@20 : 0.0869    ndcg@1 : 0.0099    ndcg@5 : 0.0229    ndcg@10 : 0.0297    ndcg@20 : 0.0371    mrr@1 : 0.0099    mrr@5 : 0.0186    mrr@10 : 0.0214    mrr@20 : 0.0234
11 Feb 23:22    INFO  epoch 28 training [time: 44.34s, train loss: 5153.4078]
11 Feb 23:22    INFO  epoch 28 evaluating [time: 1.49s, valid_score: 0.028900]
11 Feb 23:22    INFO  valid result: 
hit@1 : 0.0095    hit@5 : 0.0338    hit@10 : 0.0562    hit@20 : 0.0836    ndcg@1 : 0.0095    ndcg@5 : 0.0217    ndcg@10 : 0.0289    ndcg@20 : 0.0358    mrr@1 : 0.0095    mrr@5 : 0.0178    mrr@10 : 0.0207    mrr@20 : 0.0226
11 Feb 23:23    INFO  epoch 29 training [time: 44.30s, train loss: 5177.0885]
11 Feb 23:23    INFO  epoch 29 evaluating [time: 1.44s, valid_score: 0.030400]
11 Feb 23:23    INFO  valid result: 
hit@1 : 0.0094    hit@5 : 0.0371    hit@10 : 0.0592    hit@20 : 0.0867    ndcg@1 : 0.0094    ndcg@5 : 0.0233    ndcg@10 : 0.0304    ndcg@20 : 0.0373    mrr@1 : 0.0094    mrr@5 : 0.0187    mrr@10 : 0.0217    mrr@20 : 0.0235
11 Feb 23:23    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:23    INFO  epoch 30 training [time: 44.31s, train loss: 5143.2986]
11 Feb 23:23    INFO  epoch 30 evaluating [time: 1.49s, valid_score: 0.030100]
11 Feb 23:23    INFO  valid result: 
hit@1 : 0.0103    hit@5 : 0.036    hit@10 : 0.0575    hit@20 : 0.0865    ndcg@1 : 0.0103    ndcg@5 : 0.0232    ndcg@10 : 0.0301    ndcg@20 : 0.0374    mrr@1 : 0.0103    mrr@5 : 0.019    mrr@10 : 0.0218    mrr@20 : 0.0238
11 Feb 23:24    INFO  epoch 31 training [time: 44.84s, train loss: 5107.0938]
11 Feb 23:24    INFO  epoch 31 evaluating [time: 1.44s, valid_score: 0.029100]
11 Feb 23:24    INFO  valid result: 
hit@1 : 0.0097    hit@5 : 0.0354    hit@10 : 0.0555    hit@20 : 0.0842    ndcg@1 : 0.0097    ndcg@5 : 0.0227    ndcg@10 : 0.0291    ndcg@20 : 0.0363    mrr@1 : 0.0097    mrr@5 : 0.0185    mrr@10 : 0.0211    mrr@20 : 0.0231
11 Feb 23:25    INFO  epoch 32 training [time: 44.77s, train loss: 5063.0931]
11 Feb 23:25    INFO  epoch 32 evaluating [time: 1.48s, valid_score: 0.030900]
11 Feb 23:25    INFO  valid result: 
hit@1 : 0.0107    hit@5 : 0.0383    hit@10 : 0.0584    hit@20 : 0.0882    ndcg@1 : 0.0107    ndcg@5 : 0.0244    ndcg@10 : 0.0309    ndcg@20 : 0.0383    mrr@1 : 0.0107    mrr@5 : 0.0199    mrr@10 : 0.0225    mrr@20 : 0.0246
11 Feb 23:25    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:26    INFO  epoch 33 training [time: 45.06s, train loss: 5009.9145]
11 Feb 23:26    INFO  epoch 33 evaluating [time: 1.45s, valid_score: 0.030900]
11 Feb 23:26    INFO  valid result: 
hit@1 : 0.0114    hit@5 : 0.0367    hit@10 : 0.0576    hit@20 : 0.0863    ndcg@1 : 0.0114    ndcg@5 : 0.0242    ndcg@10 : 0.0309    ndcg@20 : 0.0381    mrr@1 : 0.0114    mrr@5 : 0.0201    mrr@10 : 0.0228    mrr@20 : 0.0248
11 Feb 23:26    INFO  Saving current: outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:27    INFO  epoch 34 training [time: 45.11s, train loss: 5054.6942]
11 Feb 23:27    INFO  epoch 34 evaluating [time: 1.45s, valid_score: 0.030800]
11 Feb 23:27    INFO  valid result: 
hit@1 : 0.0108    hit@5 : 0.0354    hit@10 : 0.0588    hit@20 : 0.0868    ndcg@1 : 0.0108    ndcg@5 : 0.0233    ndcg@10 : 0.0308    ndcg@20 : 0.0378    mrr@1 : 0.0108    mrr@5 : 0.0193    mrr@10 : 0.0223    mrr@20 : 0.0242
11 Feb 23:27    INFO  epoch 35 training [time: 44.84s, train loss: 5045.3605]
11 Feb 23:27    INFO  epoch 35 evaluating [time: 1.45s, valid_score: 0.029100]
11 Feb 23:27    INFO  valid result: 
hit@1 : 0.0096    hit@5 : 0.036    hit@10 : 0.0557    hit@20 : 0.0847    ndcg@1 : 0.0096    ndcg@5 : 0.0227    ndcg@10 : 0.0291    ndcg@20 : 0.0363    mrr@1 : 0.0096    mrr@5 : 0.0184    mrr@10 : 0.021    mrr@20 : 0.023
11 Feb 23:28    INFO  epoch 36 training [time: 44.69s, train loss: 5016.4093]
11 Feb 23:28    INFO  epoch 36 evaluating [time: 1.44s, valid_score: 0.030100]
11 Feb 23:28    INFO  valid result: 
hit@1 : 0.0107    hit@5 : 0.0363    hit@10 : 0.0568    hit@20 : 0.0854    ndcg@1 : 0.0107    ndcg@5 : 0.0236    ndcg@10 : 0.0301    ndcg@20 : 0.0373    mrr@1 : 0.0107    mrr@5 : 0.0194    mrr@10 : 0.0221    mrr@20 : 0.024
11 Feb 23:29    INFO  epoch 37 training [time: 44.25s, train loss: 4970.4406]
11 Feb 23:29    INFO  epoch 37 evaluating [time: 1.49s, valid_score: 0.029800]
11 Feb 23:29    INFO  valid result: 
hit@1 : 0.0112    hit@5 : 0.0355    hit@10 : 0.0554    hit@20 : 0.0854    ndcg@1 : 0.0112    ndcg@5 : 0.0234    ndcg@10 : 0.0298    ndcg@20 : 0.0374    mrr@1 : 0.0112    mrr@5 : 0.0194    mrr@10 : 0.0221    mrr@20 : 0.0241
11 Feb 23:30    INFO  epoch 38 training [time: 44.25s, train loss: 4972.4301]
11 Feb 23:30    INFO  epoch 38 evaluating [time: 1.45s, valid_score: 0.029600]
11 Feb 23:30    INFO  valid result: 
hit@1 : 0.0097    hit@5 : 0.036    hit@10 : 0.0567    hit@20 : 0.0859    ndcg@1 : 0.0097    ndcg@5 : 0.023    ndcg@10 : 0.0296    ndcg@20 : 0.037    mrr@1 : 0.0097    mrr@5 : 0.0187    mrr@10 : 0.0214    mrr@20 : 0.0234
11 Feb 23:30    INFO  epoch 39 training [time: 44.71s, train loss: 4965.6191]
11 Feb 23:30    INFO  epoch 39 evaluating [time: 1.48s, valid_score: 0.030000]
11 Feb 23:30    INFO  valid result: 
hit@1 : 0.0106    hit@5 : 0.0364    hit@10 : 0.0564    hit@20 : 0.0848    ndcg@1 : 0.0106    ndcg@5 : 0.0235    ndcg@10 : 0.03    ndcg@20 : 0.0371    mrr@1 : 0.0106    mrr@5 : 0.0193    mrr@10 : 0.022    mrr@20 : 0.0239
11 Feb 23:31    INFO  epoch 40 training [time: 44.20s, train loss: 4982.8026]
11 Feb 23:31    INFO  epoch 40 evaluating [time: 1.45s, valid_score: 0.030300]
11 Feb 23:31    INFO  valid result: 
hit@1 : 0.0102    hit@5 : 0.0365    hit@10 : 0.0573    hit@20 : 0.0856    ndcg@1 : 0.0102    ndcg@5 : 0.0236    ndcg@10 : 0.0303    ndcg@20 : 0.0375    mrr@1 : 0.0102    mrr@5 : 0.0194    mrr@10 : 0.0222    mrr@20 : 0.0241
11 Feb 23:32    INFO  epoch 41 training [time: 44.45s, train loss: 4870.7684]
11 Feb 23:32    INFO  epoch 41 evaluating [time: 1.48s, valid_score: 0.029600]
11 Feb 23:32    INFO  valid result: 
hit@1 : 0.0102    hit@5 : 0.0349    hit@10 : 0.0562    hit@20 : 0.0818    ndcg@1 : 0.0102    ndcg@5 : 0.0228    ndcg@10 : 0.0296    ndcg@20 : 0.0361    mrr@1 : 0.0102    mrr@5 : 0.0188    mrr@10 : 0.0216    mrr@20 : 0.0233
11 Feb 23:33    INFO  epoch 42 training [time: 44.21s, train loss: 4879.3030]
11 Feb 23:33    INFO  epoch 42 evaluating [time: 1.45s, valid_score: 0.030300]
11 Feb 23:33    INFO  valid result: 
hit@1 : 0.0107    hit@5 : 0.0355    hit@10 : 0.0568    hit@20 : 0.0841    ndcg@1 : 0.0107    ndcg@5 : 0.0234    ndcg@10 : 0.0303    ndcg@20 : 0.0372    mrr@1 : 0.0107    mrr@5 : 0.0194    mrr@10 : 0.0223    mrr@20 : 0.0241
11 Feb 23:33    INFO  epoch 43 training [time: 44.30s, train loss: 4893.7390]
11 Feb 23:33    INFO  epoch 43 evaluating [time: 1.47s, valid_score: 0.030400]
11 Feb 23:33    INFO  valid result: 
hit@1 : 0.0107    hit@5 : 0.037    hit@10 : 0.0565    hit@20 : 0.0859    ndcg@1 : 0.0107    ndcg@5 : 0.0242    ndcg@10 : 0.0304    ndcg@20 : 0.0378    mrr@1 : 0.0107    mrr@5 : 0.0199    mrr@10 : 0.0225    mrr@20 : 0.0245
11 Feb 23:34    INFO  epoch 44 training [time: 44.11s, train loss: 4890.4867]
11 Feb 23:34    INFO  epoch 44 evaluating [time: 1.45s, valid_score: 0.029400]
11 Feb 23:34    INFO  valid result: 
hit@1 : 0.011    hit@5 : 0.0346    hit@10 : 0.0547    hit@20 : 0.0822    ndcg@1 : 0.011    ndcg@5 : 0.0229    ndcg@10 : 0.0294    ndcg@20 : 0.0364    mrr@1 : 0.011    mrr@5 : 0.0191    mrr@10 : 0.0218    mrr@20 : 0.0237
11 Feb 23:34    INFO  Finished training, best eval result in epoch 33
11 Feb 23:34    INFO  Loading model structure and parameters from outputs/bert4rec_beauty/BERT4Rec-Feb-11-2026_23-00-08.pth
11 Feb 23:34    INFO  The running environment of this training is as follows:
+-------------+------------------+
| Environment |      Usage       |
+=============+==================+
| CPU         |      3.30 %      |
+-------------+------------------+
| GPU         |  2.18 G/79.44 G  |
+-------------+------------------+
| Memory      | 1.58 G/2015.66 G |
+-------------+------------------+
11 Feb 23:34    INFO  best valid : OrderedDict([('hit@1', 0.0114), ('hit@5', 0.0367), ('hit@10', 0.0576), ('hit@20', 0.0863), ('ndcg@1', 0.0114), ('ndcg@5', 0.0242), ('ndcg@10', 0.0309), ('ndcg@20', 0.0381), ('mrr@1', 0.0114), ('mrr@5', 0.0201), ('mrr@10', 0.0228), ('mrr@20', 0.0248)])
11 Feb 23:34    INFO  test result: OrderedDict([('hit@1', 0.0072), ('hit@5', 0.0242), ('hit@10', 0.0404), ('hit@20', 0.0621), ('ndcg@1', 0.0072), ('ndcg@5', 0.0158), ('ndcg@10', 0.0211), ('ndcg@20', 0.0265), ('mrr@1', 0.0072), ('mrr@5', 0.0131), ('mrr@10', 0.0152), ('mrr@20', 0.0167)])
