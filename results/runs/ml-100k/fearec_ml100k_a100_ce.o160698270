=== hostname ===
gadi-dgx-a100-0001.gadi.nci.org.au
=== PBS_JOBID ===
160698270.gadi-pbs
command line args [-m FEARec -d ml-100k --config_files configs/ml-100k/ml100k_FEARec.yaml] will not be used in RecBole
12 Feb 09:12    INFO  ['RecBole/run_recbole.py', '-m', 'FEARec', '-d', 'ml-100k', '--config_files', 'configs/ml-100k/ml100k_FEARec.yaml']
12 Feb 09:12    INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2026
state = INFO
reproducibility = True
data_path = /home/572/kd6504/TRACT/RecBole/recbole/config/../dataset_example/ml-100k
checkpoint_dir = outputs/fearec_ml100k
show_progress = False
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 200
train_batch_size = 128
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = True
metrics = ['Hit', 'NDCG', 'MRR']
topk = [1, 5, 10, 20]
valid_metric = NDCG@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = {'inter': ['rating']}
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = True
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
kg_reverse_r = False
entity_kg_num_interval = None
relation_kg_num_interval = None
benchmark_filename = None

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = None
n_layers = 2
n_heads = 2
hidden_size = 64
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.5
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
loss_type = CE
lmd = 0.1
lmd_sem = 0.1
global_ratio = 1
dual_domain = False
std = False
spatial_ratio = 0
fredom = False
fredom_type = None
topk_factor = 1
numerical_features = []
discretization = None
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cuda
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:648: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=0, inplace=True)
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:650: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=feat[field].mean(), inplace=True)
12 Feb 09:12    INFO  ml-100k
The number of users: 944
Average actions of users: 105.28844114528101
The number of items: 1350
Average actions of items: 73.6004447739066
The number of inters: 99287
The sparsity of the dataset: 92.20911801632141%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp']
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:2202: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  new_data[k] = torch.LongTensor(value)
12 Feb 09:12    INFO  [Training]: train_batch_size = [128] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
12 Feb 09:12    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
1>0.5:True
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_k=26, index_k=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_v=26, index_v=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
1>0.5:True
modes_q=26, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_k=26, index_k=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
modes_v=26, index_v=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
12 Feb 09:12    INFO  FEARec(
  (item_embedding): Embedding(1350, 64, padding_idx=0)
  (position_embedding): Embedding(50, 64)
  (item_encoder): FEAEncoder(
    (layer): ModuleList(
      (0-1): 2 x FEABlock(
        (hybrid_attention): HybridAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (query_layer): Linear(in_features=64, out_features=64, bias=True)
          (key_layer): Linear(in_features=64, out_features=64, bias=True)
          (value_layer): Linear(in_features=64, out_features=64, bias=True)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (layer_ramp): FEABlock(
      (hybrid_attention): HybridAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (query_layer): Linear(in_features=64, out_features=64, bias=True)
        (key_layer): Linear(in_features=64, out_features=64, bias=True)
        (value_layer): Linear(in_features=64, out_features=64, bias=True)
        (attn_dropout): Dropout(p=0.5, inplace=False)
        (dense): Linear(in_features=64, out_features=64, bias=True)
        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
        (out_dropout): Dropout(p=0.5, inplace=False)
      )
      (feed_forward): FeedForward(
        (dense_1): Linear(in_features=64, out_features=256, bias=True)
        (dense_2): Linear(in_features=256, out_features=64, bias=True)
        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (loss_fct): CrossEntropyLoss()
  (aug_nce_fct): CrossEntropyLoss()
  (sem_aug_nce_fct): CrossEntropyLoss()
)
Trainable parameters: 189696
12 Feb 09:12    INFO  FLOPs: 12364864.0
/home/572/kd6504/TRACT/RecBole/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = amp.GradScaler(enabled=self.enable_scaler)
12 Feb 09:13    INFO  epoch 0 training [time: 51.52s, train loss: 4723.9790]
12 Feb 09:13    INFO  epoch 0 evaluating [time: 0.06s, valid_score: 0.046800]
12 Feb 09:13    INFO  valid result: 
hit@1 : 0.0106    hit@5 : 0.0573    hit@10 : 0.0997    hit@20 : 0.1782    ndcg@1 : 0.0106    ndcg@5 : 0.0332    ndcg@10 : 0.0468    ndcg@20 : 0.0664    mrr@1 : 0.0106    mrr@5 : 0.0254    mrr@10 : 0.0309    mrr@20 : 0.0362
12 Feb 09:13    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:14    INFO  epoch 1 training [time: 50.79s, train loss: 4436.6681]
12 Feb 09:14    INFO  epoch 1 evaluating [time: 0.03s, valid_score: 0.059900]
12 Feb 09:14    INFO  valid result: 
hit@1 : 0.0148    hit@5 : 0.0742    hit@10 : 0.1283    hit@20 : 0.2227    ndcg@1 : 0.0148    ndcg@5 : 0.0424    ndcg@10 : 0.0599    ndcg@20 : 0.0838    mrr@1 : 0.0148    mrr@5 : 0.0322    mrr@10 : 0.0394    mrr@20 : 0.046
12 Feb 09:14    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:15    INFO  epoch 2 training [time: 50.72s, train loss: 4377.3596]
12 Feb 09:15    INFO  epoch 2 evaluating [time: 0.03s, valid_score: 0.054600]
12 Feb 09:15    INFO  valid result: 
hit@1 : 0.0053    hit@5 : 0.0742    hit@10 : 0.123    hit@20 : 0.2142    ndcg@1 : 0.0053    ndcg@5 : 0.0388    ndcg@10 : 0.0546    ndcg@20 : 0.0773    mrr@1 : 0.0053    mrr@5 : 0.0273    mrr@10 : 0.0338    mrr@20 : 0.0399
12 Feb 09:16    INFO  epoch 3 training [time: 50.74s, train loss: 4345.2379]
12 Feb 09:16    INFO  epoch 3 evaluating [time: 0.03s, valid_score: 0.060200]
12 Feb 09:16    INFO  valid result: 
hit@1 : 0.018    hit@5 : 0.0615    hit@10 : 0.1251    hit@20 : 0.2153    ndcg@1 : 0.018    ndcg@5 : 0.0396    ndcg@10 : 0.0602    ndcg@20 : 0.0828    mrr@1 : 0.018    mrr@5 : 0.0324    mrr@10 : 0.041    mrr@20 : 0.0471
12 Feb 09:16    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:17    INFO  epoch 4 training [time: 50.79s, train loss: 4317.5248]
12 Feb 09:17    INFO  epoch 4 evaluating [time: 0.03s, valid_score: 0.058600]
12 Feb 09:17    INFO  valid result: 
hit@1 : 0.0117    hit@5 : 0.0679    hit@10 : 0.1326    hit@20 : 0.2439    ndcg@1 : 0.0117    ndcg@5 : 0.0381    ndcg@10 : 0.0586    ndcg@20 : 0.0865    mrr@1 : 0.0117    mrr@5 : 0.0286    mrr@10 : 0.0368    mrr@20 : 0.0443
12 Feb 09:17    INFO  epoch 5 training [time: 50.72s, train loss: 4299.6478]
12 Feb 09:17    INFO  epoch 5 evaluating [time: 0.02s, valid_score: 0.063500]
12 Feb 09:17    INFO  valid result: 
hit@1 : 0.0127    hit@5 : 0.0732    hit@10 : 0.1357    hit@20 : 0.2428    ndcg@1 : 0.0127    ndcg@5 : 0.0433    ndcg@10 : 0.0635    ndcg@20 : 0.0903    mrr@1 : 0.0127    mrr@5 : 0.0336    mrr@10 : 0.0418    mrr@20 : 0.049
12 Feb 09:17    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:18    INFO  epoch 6 training [time: 50.53s, train loss: 4284.2291]
12 Feb 09:18    INFO  epoch 6 evaluating [time: 0.03s, valid_score: 0.062600]
12 Feb 09:18    INFO  valid result: 
hit@1 : 0.0138    hit@5 : 0.0721    hit@10 : 0.1379    hit@20 : 0.2407    ndcg@1 : 0.0138    ndcg@5 : 0.0418    ndcg@10 : 0.0626    ndcg@20 : 0.0885    mrr@1 : 0.0138    mrr@5 : 0.032    mrr@10 : 0.0403    mrr@20 : 0.0474
12 Feb 09:19    INFO  epoch 7 training [time: 50.55s, train loss: 4273.8820]
12 Feb 09:19    INFO  epoch 7 evaluating [time: 0.02s, valid_score: 0.060200]
12 Feb 09:19    INFO  valid result: 
hit@1 : 0.0095    hit@5 : 0.07    hit@10 : 0.1368    hit@20 : 0.2322    ndcg@1 : 0.0095    ndcg@5 : 0.039    ndcg@10 : 0.0602    ndcg@20 : 0.084    mrr@1 : 0.0095    mrr@5 : 0.0289    mrr@10 : 0.0375    mrr@20 : 0.0438
12 Feb 09:20    INFO  epoch 8 training [time: 49.46s, train loss: 4265.7407]
12 Feb 09:20    INFO  epoch 8 evaluating [time: 0.03s, valid_score: 0.060200]
12 Feb 09:20    INFO  valid result: 
hit@1 : 0.0106    hit@5 : 0.0742    hit@10 : 0.1315    hit@20 : 0.2344    ndcg@1 : 0.0106    ndcg@5 : 0.0416    ndcg@10 : 0.0602    ndcg@20 : 0.086    mrr@1 : 0.0106    mrr@5 : 0.031    mrr@10 : 0.0387    mrr@20 : 0.0457
12 Feb 09:21    INFO  epoch 9 training [time: 49.25s, train loss: 4259.1111]
12 Feb 09:21    INFO  epoch 9 evaluating [time: 0.02s, valid_score: 0.066100]
12 Feb 09:21    INFO  valid result: 
hit@1 : 0.0223    hit@5 : 0.0785    hit@10 : 0.1294    hit@20 : 0.2185    ndcg@1 : 0.0223    ndcg@5 : 0.0499    ndcg@10 : 0.0661    ndcg@20 : 0.0884    mrr@1 : 0.0223    mrr@5 : 0.0407    mrr@10 : 0.0472    mrr@20 : 0.0532
12 Feb 09:21    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:22    INFO  epoch 10 training [time: 49.49s, train loss: 4249.9828]
12 Feb 09:22    INFO  epoch 10 evaluating [time: 0.03s, valid_score: 0.057400]
12 Feb 09:22    INFO  valid result: 
hit@1 : 0.0095    hit@5 : 0.0679    hit@10 : 0.1283    hit@20 : 0.2312    ndcg@1 : 0.0095    ndcg@5 : 0.0382    ndcg@10 : 0.0574    ndcg@20 : 0.0831    mrr@1 : 0.0095    mrr@5 : 0.0285    mrr@10 : 0.0363    mrr@20 : 0.0432
12 Feb 09:22    INFO  epoch 11 training [time: 49.43s, train loss: 4245.1303]
12 Feb 09:22    INFO  epoch 11 evaluating [time: 0.02s, valid_score: 0.064900]
12 Feb 09:22    INFO  valid result: 
hit@1 : 0.0095    hit@5 : 0.0732    hit@10 : 0.1463    hit@20 : 0.2195    ndcg@1 : 0.0095    ndcg@5 : 0.0415    ndcg@10 : 0.0649    ndcg@20 : 0.0833    mrr@1 : 0.0095    mrr@5 : 0.0312    mrr@10 : 0.0407    mrr@20 : 0.0457
12 Feb 09:23    INFO  epoch 12 training [time: 49.29s, train loss: 4239.2696]
12 Feb 09:23    INFO  epoch 12 evaluating [time: 0.03s, valid_score: 0.067200]
12 Feb 09:23    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0912    hit@10 : 0.1379    hit@20 : 0.2333    ndcg@1 : 0.0159    ndcg@5 : 0.0526    ndcg@10 : 0.0672    ndcg@20 : 0.0912    mrr@1 : 0.0159    mrr@5 : 0.0401    mrr@10 : 0.0459    mrr@20 : 0.0524
12 Feb 09:23    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:24    INFO  epoch 13 training [time: 49.87s, train loss: 4235.8806]
12 Feb 09:24    INFO  epoch 13 evaluating [time: 0.02s, valid_score: 0.068200]
12 Feb 09:24    INFO  valid result: 
hit@1 : 0.0148    hit@5 : 0.087    hit@10 : 0.1432    hit@20 : 0.2344    ndcg@1 : 0.0148    ndcg@5 : 0.0501    ndcg@10 : 0.0682    ndcg@20 : 0.0911    mrr@1 : 0.0148    mrr@5 : 0.0381    mrr@10 : 0.0455    mrr@20 : 0.0518
12 Feb 09:24    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:25    INFO  epoch 14 training [time: 50.48s, train loss: 4229.7198]
12 Feb 09:25    INFO  epoch 14 evaluating [time: 0.03s, valid_score: 0.068600]
12 Feb 09:25    INFO  valid result: 
hit@1 : 0.0212    hit@5 : 0.0785    hit@10 : 0.14    hit@20 : 0.2322    ndcg@1 : 0.0212    ndcg@5 : 0.0489    ndcg@10 : 0.0686    ndcg@20 : 0.0918    mrr@1 : 0.0212    mrr@5 : 0.0393    mrr@10 : 0.0473    mrr@20 : 0.0536
12 Feb 09:25    INFO  Saving current: outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:26    INFO  epoch 15 training [time: 50.56s, train loss: 4226.2279]
12 Feb 09:26    INFO  epoch 15 evaluating [time: 0.02s, valid_score: 0.065800]
12 Feb 09:26    INFO  valid result: 
hit@1 : 0.0106    hit@5 : 0.0774    hit@10 : 0.1453    hit@20 : 0.2333    ndcg@1 : 0.0106    ndcg@5 : 0.0442    ndcg@10 : 0.0658    ndcg@20 : 0.088    mrr@1 : 0.0106    mrr@5 : 0.0333    mrr@10 : 0.042    mrr@20 : 0.0481
12 Feb 09:27    INFO  epoch 16 training [time: 50.51s, train loss: 4221.7186]
12 Feb 09:27    INFO  epoch 16 evaluating [time: 0.03s, valid_score: 0.065200]
12 Feb 09:27    INFO  valid result: 
hit@1 : 0.0127    hit@5 : 0.0806    hit@10 : 0.141    hit@20 : 0.228    ndcg@1 : 0.0127    ndcg@5 : 0.0458    ndcg@10 : 0.0652    ndcg@20 : 0.0871    mrr@1 : 0.0127    mrr@5 : 0.0345    mrr@10 : 0.0425    mrr@20 : 0.0484
12 Feb 09:27    INFO  epoch 17 training [time: 50.46s, train loss: 4219.9977]
12 Feb 09:27    INFO  epoch 17 evaluating [time: 0.02s, valid_score: 0.064200]
12 Feb 09:27    INFO  valid result: 
hit@1 : 0.0127    hit@5 : 0.0764    hit@10 : 0.1474    hit@20 : 0.2418    ndcg@1 : 0.0127    ndcg@5 : 0.0415    ndcg@10 : 0.0642    ndcg@20 : 0.0879    mrr@1 : 0.0127    mrr@5 : 0.0304    mrr@10 : 0.0397    mrr@20 : 0.0461
12 Feb 09:28    INFO  epoch 18 training [time: 50.73s, train loss: 4215.1620]
12 Feb 09:28    INFO  epoch 18 evaluating [time: 0.03s, valid_score: 0.064100]
12 Feb 09:28    INFO  valid result: 
hit@1 : 0.0127    hit@5 : 0.0764    hit@10 : 0.141    hit@20 : 0.2333    ndcg@1 : 0.0127    ndcg@5 : 0.0434    ndcg@10 : 0.0641    ndcg@20 : 0.0874    mrr@1 : 0.0127    mrr@5 : 0.0328    mrr@10 : 0.0412    mrr@20 : 0.0475
12 Feb 09:29    INFO  epoch 19 training [time: 50.64s, train loss: 4211.6165]
12 Feb 09:29    INFO  epoch 19 evaluating [time: 0.03s, valid_score: 0.064500]
12 Feb 09:29    INFO  valid result: 
hit@1 : 0.0117    hit@5 : 0.0785    hit@10 : 0.141    hit@20 : 0.2322    ndcg@1 : 0.0117    ndcg@5 : 0.0446    ndcg@10 : 0.0645    ndcg@20 : 0.0876    mrr@1 : 0.0117    mrr@5 : 0.0335    mrr@10 : 0.0416    mrr@20 : 0.0479
12 Feb 09:30    INFO  epoch 20 training [time: 50.67s, train loss: 4208.1392]
12 Feb 09:30    INFO  epoch 20 evaluating [time: 0.02s, valid_score: 0.068400]
12 Feb 09:30    INFO  valid result: 
hit@1 : 0.0201    hit@5 : 0.0806    hit@10 : 0.1389    hit@20 : 0.2375    ndcg@1 : 0.0201    ndcg@5 : 0.0498    ndcg@10 : 0.0684    ndcg@20 : 0.0928    mrr@1 : 0.0201    mrr@5 : 0.0398    mrr@10 : 0.0473    mrr@20 : 0.0537
12 Feb 09:31    INFO  epoch 21 training [time: 50.66s, train loss: 4204.2551]
12 Feb 09:31    INFO  epoch 21 evaluating [time: 0.03s, valid_score: 0.068200]
12 Feb 09:31    INFO  valid result: 
hit@1 : 0.018    hit@5 : 0.0774    hit@10 : 0.1463    hit@20 : 0.2503    ndcg@1 : 0.018    ndcg@5 : 0.0464    ndcg@10 : 0.0682    ndcg@20 : 0.0943    mrr@1 : 0.018    mrr@5 : 0.0364    mrr@10 : 0.0451    mrr@20 : 0.0522
12 Feb 09:32    INFO  epoch 22 training [time: 50.68s, train loss: 4204.8357]
12 Feb 09:32    INFO  epoch 22 evaluating [time: 0.02s, valid_score: 0.062400]
12 Feb 09:32    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0647    hit@10 : 0.1368    hit@20 : 0.2428    ndcg@1 : 0.0159    ndcg@5 : 0.0393    ndcg@10 : 0.0624    ndcg@20 : 0.0889    mrr@1 : 0.0159    mrr@5 : 0.0311    mrr@10 : 0.0406    mrr@20 : 0.0477
12 Feb 09:33    INFO  epoch 23 training [time: 50.64s, train loss: 4199.9039]
12 Feb 09:33    INFO  epoch 23 evaluating [time: 0.03s, valid_score: 0.065200]
12 Feb 09:33    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0689    hit@10 : 0.1421    hit@20 : 0.2428    ndcg@1 : 0.0159    ndcg@5 : 0.0419    ndcg@10 : 0.0652    ndcg@20 : 0.0905    mrr@1 : 0.0159    mrr@5 : 0.0331    mrr@10 : 0.0425    mrr@20 : 0.0493
12 Feb 09:33    INFO  epoch 24 training [time: 50.69s, train loss: 4197.8224]
12 Feb 09:33    INFO  epoch 24 evaluating [time: 0.02s, valid_score: 0.060400]
12 Feb 09:33    INFO  valid result: 
hit@1 : 0.0085    hit@5 : 0.0679    hit@10 : 0.1357    hit@20 : 0.2291    ndcg@1 : 0.0085    ndcg@5 : 0.0385    ndcg@10 : 0.0604    ndcg@20 : 0.0838    mrr@1 : 0.0085    mrr@5 : 0.0289    mrr@10 : 0.0379    mrr@20 : 0.0442
12 Feb 09:34    INFO  epoch 25 training [time: 50.73s, train loss: 4195.7300]
12 Feb 09:34    INFO  epoch 25 evaluating [time: 0.03s, valid_score: 0.063100]
12 Feb 09:34    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0732    hit@10 : 0.1357    hit@20 : 0.2174    ndcg@1 : 0.0159    ndcg@5 : 0.0431    ndcg@10 : 0.0631    ndcg@20 : 0.0836    mrr@1 : 0.0159    mrr@5 : 0.0335    mrr@10 : 0.0416    mrr@20 : 0.0471
12 Feb 09:34    INFO  Finished training, best eval result in epoch 14
12 Feb 09:34    INFO  Loading model structure and parameters from outputs/fearec_ml100k/FEARec-Feb-12-2026_09-12-48.pth
12 Feb 09:34    INFO  The running environment of this training is as follows:
+-------------+------------------+
| Environment |      Usage       |
+=============+==================+
| CPU         |      1.70 %      |
+-------------+------------------+
| GPU         |  0.64 G/79.44 G  |
+-------------+------------------+
| Memory      | 1.45 G/2015.66 G |
+-------------+------------------+
12 Feb 09:34    INFO  best valid : OrderedDict([('hit@1', 0.0212), ('hit@5', 0.0785), ('hit@10', 0.14), ('hit@20', 0.2322), ('ndcg@1', 0.0212), ('ndcg@5', 0.0489), ('ndcg@10', 0.0686), ('ndcg@20', 0.0918), ('mrr@1', 0.0212), ('mrr@5', 0.0393), ('mrr@10', 0.0473), ('mrr@20', 0.0536)])
12 Feb 09:34    INFO  test result: OrderedDict([('hit@1', 0.0159), ('hit@5', 0.07), ('hit@10', 0.1304), ('hit@20', 0.211), ('ndcg@1', 0.0159), ('ndcg@5', 0.0426), ('ndcg@10', 0.0622), ('ndcg@20', 0.0824), ('mrr@1', 0.0159), ('mrr@5', 0.0337), ('mrr@10', 0.0418), ('mrr@20', 0.0472)])
Job completed.

======================================================================================
                  Resource Usage on 2026-02-12 09:34:47:
   Job Id:             160698270.gadi-pbs
   Project:            up63
   Exit Status:        0
   Service Units:      26.56
   NCPUs Requested:    16                  CPU Time Used: 00:22:05        
   Memory Requested:   16.0GB                Memory Used: 1.19GB          
   NGPUs Requested:    1                 GPU Utilisation: 14%             
                                         GPU Memory Used: 1.15GB          
   Walltime Requested: 04:00:00            Walltime Used: 00:22:08        
   JobFS Requested:    100.0MB                JobFS Used: 5.03KB          
======================================================================================
