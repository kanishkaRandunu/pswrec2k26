command line args [-m BERT4Rec -d ml-100k --config_files configs/ml-100k/ml100k_BERT4Rec.yaml] will not be used in RecBole
12 Feb 09:06    INFO  ['RecBole/run_recbole.py', '-m', 'BERT4Rec', '-d', 'ml-100k', '--config_files', 'configs/ml-100k/ml100k_BERT4Rec.yaml']
12 Feb 09:06    INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2026
state = INFO
reproducibility = True
data_path = /home/572/kd6504/TRACT/RecBole/recbole/config/../dataset_example/ml-100k
checkpoint_dir = outputs/bert4rec_ml100k
show_progress = False
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 200
train_batch_size = 128
learner = adam
learning_rate = 0.001
train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}
repeatable = True
metrics = ['Hit', 'NDCG', 'MRR']
topk = [1, 5, 10, 20]
valid_metric = NDCG@10
valid_metric_bigger = True
eval_batch_size = 4096
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp']}
unload_col = None
unused_col = {'inter': ['rating']}
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = True
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
kg_reverse_r = False
entity_kg_num_interval = None
relation_kg_num_interval = None
benchmark_filename = None

Other Hyper Parameters: 
worker = 0
wandb_project = recbole
shuffle = True
require_pow = False
enable_amp = False
enable_scaler = False
transform = mask_itemseq
n_layers = 2
n_heads = 2
hidden_size = 64
inner_size = 256
hidden_dropout_prob = 0.2
attn_dropout_prob = 0.2
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
mask_ratio = 0.2
loss_type = CE
ft_ratio = 0.5
numerical_features = []
discretization = None
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
single_spec = True
local_rank = 0
device = cuda
valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}
test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}


/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:648: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=0, inplace=True)
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:650: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.
Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.

See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html
  feat[field].fillna(value=feat[field].mean(), inplace=True)
12 Feb 09:06    INFO  ml-100k
The number of users: 944
Average actions of users: 105.28844114528101
The number of items: 1350
Average actions of items: 73.6004447739066
The number of inters: 99287
The sparsity of the dataset: 92.20911801632141%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp']
/home/572/kd6504/TRACT/RecBole/recbole/data/dataset/dataset.py:2202: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  new_data[k] = torch.LongTensor(value)
12 Feb 09:06    INFO  [Training]: train_batch_size = [128] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]
12 Feb 09:06    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]
12 Feb 09:06    INFO  BERT4Rec(
  (item_embedding): Embedding(1351, 64, padding_idx=0)
  (position_embedding): Embedding(50, 64)
  (trm_encoder): TransformerEncoder(
    (layer): ModuleList(
      (0-1): 2 x TransformerLayer(
        (multi_head_attention): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.2, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.2, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (output_ffn): Linear(in_features=64, out_features=64, bias=True)
  (output_gelu): GELU(approximate='none')
  (output_ln): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
)
Trainable parameters: 195398
12 Feb 09:06    INFO  FLOPs: 5194664.0
/home/572/kd6504/TRACT/RecBole/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = amp.GradScaler(enabled=self.enable_scaler)
12 Feb 09:07    INFO  epoch 0 training [time: 32.31s, train loss: 4663.2387]
12 Feb 09:07    INFO  epoch 0 evaluating [time: 0.09s, valid_score: 0.053300]
12 Feb 09:07    INFO  valid result: 
hit@1 : 0.0138    hit@5 : 0.0604    hit@10 : 0.1103    hit@20 : 0.1866    ndcg@1 : 0.0138    ndcg@5 : 0.0374    ndcg@10 : 0.0533    ndcg@20 : 0.0726    mrr@1 : 0.0138    mrr@5 : 0.0299    mrr@10 : 0.0363    mrr@20 : 0.0416
12 Feb 09:07    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:07    INFO  epoch 1 training [time: 32.16s, train loss: 4375.7021]
12 Feb 09:07    INFO  epoch 1 evaluating [time: 0.07s, valid_score: 0.047100]
12 Feb 09:07    INFO  valid result: 
hit@1 : 0.0106    hit@5 : 0.0551    hit@10 : 0.0997    hit@20 : 0.2142    ndcg@1 : 0.0106    ndcg@5 : 0.0327    ndcg@10 : 0.0471    ndcg@20 : 0.0754    mrr@1 : 0.0106    mrr@5 : 0.0253    mrr@10 : 0.0313    mrr@20 : 0.0387
12 Feb 09:08    INFO  epoch 2 training [time: 32.01s, train loss: 4309.7879]
12 Feb 09:08    INFO  epoch 2 evaluating [time: 0.07s, valid_score: 0.055800]
12 Feb 09:08    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0668    hit@10 : 0.1145    hit@20 : 0.2121    ndcg@1 : 0.0159    ndcg@5 : 0.041    ndcg@10 : 0.0558    ndcg@20 : 0.08    mrr@1 : 0.0159    mrr@5 : 0.0326    mrr@10 : 0.0384    mrr@20 : 0.0448
12 Feb 09:08    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:08    INFO  epoch 3 training [time: 31.82s, train loss: 4272.0133]
12 Feb 09:08    INFO  epoch 3 evaluating [time: 0.07s, valid_score: 0.055200]
12 Feb 09:08    INFO  valid result: 
hit@1 : 0.018    hit@5 : 0.0604    hit@10 : 0.1103    hit@20 : 0.2121    ndcg@1 : 0.018    ndcg@5 : 0.0391    ndcg@10 : 0.0552    ndcg@20 : 0.0809    mrr@1 : 0.018    mrr@5 : 0.0322    mrr@10 : 0.0388    mrr@20 : 0.0458
12 Feb 09:09    INFO  epoch 4 training [time: 31.65s, train loss: 4242.4044]
12 Feb 09:09    INFO  epoch 4 evaluating [time: 0.07s, valid_score: 0.057500]
12 Feb 09:09    INFO  valid result: 
hit@1 : 0.0148    hit@5 : 0.0626    hit@10 : 0.123    hit@20 : 0.2227    ndcg@1 : 0.0148    ndcg@5 : 0.0383    ndcg@10 : 0.0575    ndcg@20 : 0.0824    mrr@1 : 0.0148    mrr@5 : 0.0305    mrr@10 : 0.0381    mrr@20 : 0.0448
12 Feb 09:09    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:09    INFO  epoch 5 training [time: 31.50s, train loss: 4217.9814]
12 Feb 09:09    INFO  epoch 5 evaluating [time: 0.06s, valid_score: 0.059700]
12 Feb 09:09    INFO  valid result: 
hit@1 : 0.017    hit@5 : 0.0679    hit@10 : 0.122    hit@20 : 0.2227    ndcg@1 : 0.017    ndcg@5 : 0.0425    ndcg@10 : 0.0597    ndcg@20 : 0.085    mrr@1 : 0.017    mrr@5 : 0.0342    mrr@10 : 0.0412    mrr@20 : 0.048
12 Feb 09:09    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:10    INFO  epoch 6 training [time: 31.57s, train loss: 4200.1729]
12 Feb 09:10    INFO  epoch 6 evaluating [time: 0.06s, valid_score: 0.057000]
12 Feb 09:10    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0668    hit@10 : 0.1156    hit@20 : 0.2216    ndcg@1 : 0.0159    ndcg@5 : 0.0415    ndcg@10 : 0.057    ndcg@20 : 0.0837    mrr@1 : 0.0159    mrr@5 : 0.0333    mrr@10 : 0.0394    mrr@20 : 0.0468
12 Feb 09:11    INFO  epoch 7 training [time: 31.46s, train loss: 4174.3580]
12 Feb 09:11    INFO  epoch 7 evaluating [time: 0.07s, valid_score: 0.062500]
12 Feb 09:11    INFO  valid result: 
hit@1 : 0.0148    hit@5 : 0.0742    hit@10 : 0.1357    hit@20 : 0.2375    ndcg@1 : 0.0148    ndcg@5 : 0.043    ndcg@10 : 0.0625    ndcg@20 : 0.088    mrr@1 : 0.0148    mrr@5 : 0.0329    mrr@10 : 0.0407    mrr@20 : 0.0476
12 Feb 09:11    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:11    INFO  epoch 8 training [time: 31.39s, train loss: 4168.7077]
12 Feb 09:11    INFO  epoch 8 evaluating [time: 0.06s, valid_score: 0.060600]
12 Feb 09:11    INFO  valid result: 
hit@1 : 0.0191    hit@5 : 0.0636    hit@10 : 0.1273    hit@20 : 0.2269    ndcg@1 : 0.0191    ndcg@5 : 0.0402    ndcg@10 : 0.0606    ndcg@20 : 0.0858    mrr@1 : 0.0191    mrr@5 : 0.0326    mrr@10 : 0.0409    mrr@20 : 0.0479
12 Feb 09:12    INFO  epoch 9 training [time: 31.61s, train loss: 4151.8366]
12 Feb 09:12    INFO  epoch 9 evaluating [time: 0.07s, valid_score: 0.061400]
12 Feb 09:12    INFO  valid result: 
hit@1 : 0.0138    hit@5 : 0.071    hit@10 : 0.1304    hit@20 : 0.246    ndcg@1 : 0.0138    ndcg@5 : 0.0425    ndcg@10 : 0.0614    ndcg@20 : 0.0905    mrr@1 : 0.0138    mrr@5 : 0.0332    mrr@10 : 0.0408    mrr@20 : 0.0487
12 Feb 09:12    INFO  epoch 10 training [time: 31.48s, train loss: 4143.8843]
12 Feb 09:12    INFO  epoch 10 evaluating [time: 0.07s, valid_score: 0.068500]
12 Feb 09:12    INFO  valid result: 
hit@1 : 0.017    hit@5 : 0.0817    hit@10 : 0.14    hit@20 : 0.228    ndcg@1 : 0.017    ndcg@5 : 0.0497    ndcg@10 : 0.0685    ndcg@20 : 0.0907    mrr@1 : 0.017    mrr@5 : 0.0392    mrr@10 : 0.047    mrr@20 : 0.053
12 Feb 09:12    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:13    INFO  epoch 11 training [time: 31.85s, train loss: 4132.3299]
12 Feb 09:13    INFO  epoch 11 evaluating [time: 0.07s, valid_score: 0.069400]
12 Feb 09:13    INFO  valid result: 
hit@1 : 0.017    hit@5 : 0.0859    hit@10 : 0.1421    hit@20 : 0.2503    ndcg@1 : 0.017    ndcg@5 : 0.0514    ndcg@10 : 0.0694    ndcg@20 : 0.0966    mrr@1 : 0.017    mrr@5 : 0.0402    mrr@10 : 0.0475    mrr@20 : 0.0549
12 Feb 09:13    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:13    INFO  epoch 12 training [time: 32.37s, train loss: 4120.4983]
12 Feb 09:13    INFO  epoch 12 evaluating [time: 0.06s, valid_score: 0.073000]
12 Feb 09:13    INFO  valid result: 
hit@1 : 0.0191    hit@5 : 0.0848    hit@10 : 0.1538    hit@20 : 0.2471    ndcg@1 : 0.0191    ndcg@5 : 0.0511    ndcg@10 : 0.073    ndcg@20 : 0.0964    mrr@1 : 0.0191    mrr@5 : 0.0402    mrr@10 : 0.049    mrr@20 : 0.0553
12 Feb 09:13    INFO  Saving current: outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:14    INFO  epoch 13 training [time: 32.13s, train loss: 4108.4607]
12 Feb 09:14    INFO  epoch 13 evaluating [time: 0.06s, valid_score: 0.070500]
12 Feb 09:14    INFO  valid result: 
hit@1 : 0.0191    hit@5 : 0.0785    hit@10 : 0.1474    hit@20 : 0.2333    ndcg@1 : 0.0191    ndcg@5 : 0.0482    ndcg@10 : 0.0705    ndcg@20 : 0.0918    mrr@1 : 0.0191    mrr@5 : 0.0384    mrr@10 : 0.0476    mrr@20 : 0.0533
12 Feb 09:14    INFO  epoch 14 training [time: 32.33s, train loss: 4101.8031]
12 Feb 09:14    INFO  epoch 14 evaluating [time: 0.07s, valid_score: 0.066100]
12 Feb 09:14    INFO  valid result: 
hit@1 : 0.0212    hit@5 : 0.0764    hit@10 : 0.1315    hit@20 : 0.2312    ndcg@1 : 0.0212    ndcg@5 : 0.0486    ndcg@10 : 0.0661    ndcg@20 : 0.0912    mrr@1 : 0.0212    mrr@5 : 0.0396    mrr@10 : 0.0466    mrr@20 : 0.0534
12 Feb 09:15    INFO  epoch 15 training [time: 32.13s, train loss: 4095.9085]
12 Feb 09:15    INFO  epoch 15 evaluating [time: 0.07s, valid_score: 0.065200]
12 Feb 09:15    INFO  valid result: 
hit@1 : 0.0127    hit@5 : 0.0817    hit@10 : 0.1379    hit@20 : 0.2407    ndcg@1 : 0.0127    ndcg@5 : 0.0472    ndcg@10 : 0.0652    ndcg@20 : 0.0909    mrr@1 : 0.0127    mrr@5 : 0.0359    mrr@10 : 0.0433    mrr@20 : 0.0502
12 Feb 09:15    INFO  epoch 16 training [time: 32.12s, train loss: 4085.2325]
12 Feb 09:15    INFO  epoch 16 evaluating [time: 0.06s, valid_score: 0.070500]
12 Feb 09:15    INFO  valid result: 
hit@1 : 0.0223    hit@5 : 0.0806    hit@10 : 0.1421    hit@20 : 0.2248    ndcg@1 : 0.0223    ndcg@5 : 0.0506    ndcg@10 : 0.0705    ndcg@20 : 0.0911    mrr@1 : 0.0223    mrr@5 : 0.041    mrr@10 : 0.0491    mrr@20 : 0.0546
12 Feb 09:16    INFO  epoch 17 training [time: 32.30s, train loss: 4079.2629]
12 Feb 09:16    INFO  epoch 17 evaluating [time: 0.06s, valid_score: 0.069600]
12 Feb 09:16    INFO  valid result: 
hit@1 : 0.0265    hit@5 : 0.0774    hit@10 : 0.1336    hit@20 : 0.2259    ndcg@1 : 0.0265    ndcg@5 : 0.0515    ndcg@10 : 0.0696    ndcg@20 : 0.0926    mrr@1 : 0.0265    mrr@5 : 0.0431    mrr@10 : 0.0505    mrr@20 : 0.0567
12 Feb 09:16    INFO  epoch 18 training [time: 32.12s, train loss: 4066.8836]
12 Feb 09:16    INFO  epoch 18 evaluating [time: 0.06s, valid_score: 0.071700]
12 Feb 09:16    INFO  valid result: 
hit@1 : 0.0212    hit@5 : 0.0901    hit@10 : 0.1474    hit@20 : 0.2418    ndcg@1 : 0.0212    ndcg@5 : 0.0535    ndcg@10 : 0.0717    ndcg@20 : 0.0953    mrr@1 : 0.0212    mrr@5 : 0.0417    mrr@10 : 0.0491    mrr@20 : 0.0554
12 Feb 09:17    INFO  epoch 19 training [time: 32.29s, train loss: 4061.1705]
12 Feb 09:17    INFO  epoch 19 evaluating [time: 0.06s, valid_score: 0.067800]
12 Feb 09:17    INFO  valid result: 
hit@1 : 0.0223    hit@5 : 0.0827    hit@10 : 0.1304    hit@20 : 0.2407    ndcg@1 : 0.0223    ndcg@5 : 0.0522    ndcg@10 : 0.0678    ndcg@20 : 0.0956    mrr@1 : 0.0223    mrr@5 : 0.0423    mrr@10 : 0.0488    mrr@20 : 0.0564
12 Feb 09:18    INFO  epoch 20 training [time: 32.13s, train loss: 4054.8871]
12 Feb 09:18    INFO  epoch 20 evaluating [time: 0.06s, valid_score: 0.067800]
12 Feb 09:18    INFO  valid result: 
hit@1 : 0.0159    hit@5 : 0.0859    hit@10 : 0.1421    hit@20 : 0.2344    ndcg@1 : 0.0159    ndcg@5 : 0.0495    ndcg@10 : 0.0678    ndcg@20 : 0.0909    mrr@1 : 0.0159    mrr@5 : 0.0377    mrr@10 : 0.0453    mrr@20 : 0.0516
12 Feb 09:18    INFO  epoch 21 training [time: 32.10s, train loss: 4046.9013]
12 Feb 09:18    INFO  epoch 21 evaluating [time: 0.06s, valid_score: 0.068900]
12 Feb 09:18    INFO  valid result: 
hit@1 : 0.018    hit@5 : 0.0785    hit@10 : 0.1442    hit@20 : 0.2375    ndcg@1 : 0.018    ndcg@5 : 0.0478    ndcg@10 : 0.0689    ndcg@20 : 0.0923    mrr@1 : 0.018    mrr@5 : 0.0378    mrr@10 : 0.0465    mrr@20 : 0.0528
12 Feb 09:19    INFO  epoch 22 training [time: 32.23s, train loss: 4047.9480]
12 Feb 09:19    INFO  epoch 22 evaluating [time: 0.06s, valid_score: 0.069000]
12 Feb 09:19    INFO  valid result: 
hit@1 : 0.0191    hit@5 : 0.0785    hit@10 : 0.141    hit@20 : 0.2492    ndcg@1 : 0.0191    ndcg@5 : 0.0487    ndcg@10 : 0.069    ndcg@20 : 0.0962    mrr@1 : 0.0191    mrr@5 : 0.039    mrr@10 : 0.0474    mrr@20 : 0.0548
12 Feb 09:19    INFO  epoch 23 training [time: 32.20s, train loss: 4041.7347]
12 Feb 09:19    INFO  epoch 23 evaluating [time: 0.07s, valid_score: 0.066800]
12 Feb 09:19    INFO  valid result: 
hit@1 : 0.0191    hit@5 : 0.0827    hit@10 : 0.1379    hit@20 : 0.2439    ndcg@1 : 0.0191    ndcg@5 : 0.0494    ndcg@10 : 0.0668    ndcg@20 : 0.0936    mrr@1 : 0.0191    mrr@5 : 0.0387    mrr@10 : 0.0456    mrr@20 : 0.0529
12 Feb 09:19    INFO  Finished training, best eval result in epoch 12
12 Feb 09:19    INFO  Loading model structure and parameters from outputs/bert4rec_ml100k/BERT4Rec-Feb-12-2026_09-06-46.pth
12 Feb 09:19    INFO  The running environment of this training is as follows:
+-------------+------------------+
| Environment |      Usage       |
+=============+==================+
| CPU         |      1.20 %      |
+-------------+------------------+
| GPU         |  0.47 G/79.44 G  |
+-------------+------------------+
| Memory      | 1.41 G/2015.66 G |
+-------------+------------------+
12 Feb 09:19    INFO  best valid : OrderedDict([('hit@1', 0.0191), ('hit@5', 0.0848), ('hit@10', 0.1538), ('hit@20', 0.2471), ('ndcg@1', 0.0191), ('ndcg@5', 0.0511), ('ndcg@10', 0.073), ('ndcg@20', 0.0964), ('mrr@1', 0.0191), ('mrr@5', 0.0402), ('mrr@10', 0.049), ('mrr@20', 0.0553)])
12 Feb 09:19    INFO  test result: OrderedDict([('hit@1', 0.0148), ('hit@5', 0.0689), ('hit@10', 0.1357), ('hit@20', 0.2375), ('ndcg@1', 0.0148), ('ndcg@5', 0.0412), ('ndcg@10', 0.0624), ('ndcg@20', 0.0879), ('mrr@1', 0.0148), ('mrr@5', 0.0322), ('mrr@10', 0.0407), ('mrr@20', 0.0476)])
